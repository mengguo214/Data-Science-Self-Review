{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Approximation Generalization Tradeoff\n",
    "\n",
    "** For Classification Problem Settings **\n",
    "\n",
    "\n",
    "$$E_{out} (g) \\leqslant E_{in} (g) + \\mathcal{O}(\\sqrt{d_{vc}\\frac{log(n)}{n}})$$\n",
    "\n",
    "**$E_{in} (g)$ decreases as $d_{vc}$ increases**;\n",
    "\n",
    "**$\\mathcal{O}(\\sqrt{d_{vc}\\frac{log(n)}{n}})$ increases as $d_{vc}$ increases**.\n",
    "\n",
    "\n",
    "$d_{vc}$ represents VC-Dimension, the greatest number of points that can be\n",
    "shattered by Hypothesis set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias-Variance Tradeoff\n",
    "\n",
    "** For Regression Problem Settings **\n",
    "\n",
    "Starts from Mean Squared Error, with training input D:\n",
    "\n",
    "$$E_{out}(g_D) = \\mathbb E_{\\overrightarrow{x}\\sim P}(g_D(\\overrightarrow{x}) - f(\\overrightarrow{x})^2)$$\n",
    "\n",
    "Thus we have:\n",
    "\n",
    "\n",
    "$$\\mathbb E[E_{out}(g)] = \\mathbb E_{\\overrightarrow x}[\\mathbb E_D[g_D(\\overrightarrow x)^2 - \\bar g(\\overrightarrow{x})^2] + (\\bar g(\\overrightarrow{x}) - f(\\overrightarrow{x})^2]$$\n",
    "\n",
    "$$= \\mathbb E_{\\overrightarrow x}[Variance\\space of\\space g_D(\\overrightarrow x) + Bias\\space of\\space \\bar g(\\overrightarrow{x})]$$\n",
    "\n",
    "\n",
    "**Bias**\n",
    "\n",
    "How different between the average of our hypothesis set and the true function f\n",
    "\n",
    "(How well, on average, does g approximate f?)\n",
    "\n",
    "\n",
    "**Vairance**\n",
    "\n",
    "How variable our whole dataset compared to the average of our hypothesis set\n",
    "\n",
    "(How well could g approximate anything? How much could noise affect g?)\n",
    "\n",
    "\n",
    "**More complicated Hypothesis**\n",
    "\n",
    "Bias decreases, Variance increases\n",
    "\n",
    "\n",
    "**More data brought to exsisting Hypothesis**\n",
    "\n",
    "Bias is fixed when model was chosen, so no change for Bias;\n",
    "\n",
    "Variance will decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "## Maximum Likelihood Estimation \n",
    "\n",
    "Maximum Likelihood Estimation (MLE) is a method of estimating the parameters of a distribution by maximizing a likelihood function, so that under the assumed statistical model the observed data is most probable. \n",
    "Finding the parameter values that maximize the likelihood of making the observations given the parameters.\n",
    "\n",
    "\n",
    "**Assumption for Linear Regression - Gaussian Noise Distribution** \n",
    "\n",
    "1. The distribution of $X$ is arbitrary \n",
    "2. If $\\overrightarrow X$ and $\\overrightarrow \\beta$ , then $Y = \\overrightarrow \\beta * \\overrightarrow X + \\epsilon$\n",
    "3. $\\epsilon \\sim Normal (0, \\sigma ^2)$\n",
    "4. $\\epsilon$ is independent across observations\n",
    "5. $Y$ is independent across observations give $X_i$\n",
    "6. Then $Y \\sim Normal ( {\\hat {\\beta}\\overrightarrow X_i}, \\sigma ^2)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Probability Density Function for Normal Distribution**\n",
    "\n",
    "$$P(x_i\\mid \\mu,\\sigma^2)={\\frac {1}{\\sqrt {2\\pi \\sigma ^{2}}}}e^{-{\\frac {(x_i-\\mu )^{2}}{2\\sigma ^{2}}}}$$\n",
    "\n",
    "** Bayes's Theorem **\n",
    "\n",
    "$$ P(\\theta \\mid X)={\\frac {P(X \\mid \\theta)P(\\theta)}{P(X)}}\\cdot $$\n",
    "\n",
    "where likelihood funtion is $P(X \\mid \\theta)$ and prior distribution is $p(\\theta )$\n",
    "\n",
    "** For a Dataset, if independence holds **\n",
    "\n",
    "$$ L = P(Y_i \\mid \\overrightarrow X) = P(Y_1,Y_2,...Y_n  \\mid \\overrightarrow X) = P(Y_1\\mid x_1)P(Y_2 \\mid x_2)P(Y_3\\mid x_3)...P(Y_n\\mid x_n) = \\prod_{i=1} ^n P(Y_i \\mid x_i) $$\n",
    "\n",
    "substitute for normal PDF\n",
    "\n",
    "$$ L = \\frac{1}{\\sigma ^n(2\\pi)^{\\frac{n}{2}}} e^{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(\\overrightarrow {\\beta}\\overrightarrow X_i)}$$\n",
    "\n",
    "$$ \\ln(L) = -n\\ln(\\sqrt {2\\pi} \\sigma) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n(Y_i- \\overrightarrow {\\beta}\\overrightarrow X_i)^2$$\n",
    "\n",
    "In order to maximize $\\ln(L)$:\n",
    "\n",
    "$$argmax_\\beta[- \\sum_{i=1}^n(Y_i- \\overrightarrow {\\beta}\\overrightarrow X_i)^2]$$\n",
    "\n",
    "Thus, $argmin_\\beta[\\sum_{i=1}^n(Y_i-\\overrightarrow {\\beta}\\overrightarrow X_i)^2]$\n",
    "\n",
    "*which is also the squared error*\n",
    "\n",
    "In order to find the optima, we can set the first derivative equals to zero, find the weights, and then check the sign using second derivative\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Squared Error and Vectorized Optimization\n",
    "\n",
    "Similarily as MLE methods, we want to optimize parameters to have lowest mean squared error.\n",
    "\n",
    "Let's try a vectorized optimization:\n",
    "\n",
    "$$ E_{in}(\\overrightarrow \\beta)=\\frac{1}{n}\\sum_{i=1}^n(\\overrightarrow {\\beta}^T \\overrightarrow {x_i} - Y_i)^2 = \\frac{1}{n}\\sum_{i=1}^n(\\overrightarrow {x_i}^T \\overrightarrow {\\beta} - Y_i)^2 = \\frac{1}{n}\\mid\\mid X\\overrightarrow w - \\overrightarrow Y \\mid \\mid ^2$$\n",
    "\n",
    "$$ since \\mid \\mid \\overrightarrow Z \\mid\\mid = \\sqrt{\\sum_{i=1}^d z_i^2} = \\sqrt{\\overrightarrow Z^T \\overrightarrow Z}$$\n",
    "\n",
    "$$ E_{in}(\\overrightarrow \\beta)= \\frac{1}{n}(X\\overrightarrow \\beta - \\overrightarrow Y)^T(X\\overrightarrow \\beta - \\overrightarrow Y)$$\n",
    "\n",
    "$$ \\Delta_\\beta E_{in}(\\overrightarrow \\beta^*) = \\frac{1}{n}(2X^TX\\overrightarrow \\beta^* - 2X^T\\overrightarrow Y) = 0$$\n",
    "\n",
    "$$\\overrightarrow \\beta^* = (X^TX)^{-1}X^T\\overrightarrow Y$$\n",
    "\n",
    "*when the number of parameter << the dataset, we can assume X is transferable*\n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "**Reference**\n",
    "\n",
    "Learning From Data (http://amlbook.com/)\n",
    "\n",
    "Maximum Likelihood Wikipedia https://en.wikipedia.org/wiki/Maximum_likelihood_estimation\n",
    "\n",
    "Estimating simple linear regression II https://www.stat.cmu.edu/~cshalizi/mreg/15/lectures/06/lecture-06.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
