{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Approximation Generalization Tradeoff\n",
    "\n",
    "** For Classification Problem Settings **\n",
    "\n",
    "\n",
    "$$E_{out} (g) \\leqslant E_{in} (g) + \\mathcal{O}(\\sqrt{d_{vc}\\frac{log(n)}{n}})$$\n",
    "\n",
    "**$E_{in} (g)$ decreases as $d_{vc}$ increases**;\n",
    "\n",
    "**$\\mathcal{O}(\\sqrt{d_{vc}\\frac{log(n)}{n}})$ increases as $d_{vc}$ increases**.\n",
    "\n",
    "\n",
    "$d_{vc}$ represents VC-Dimension, the greatest number of points that can be\n",
    "shattered by Hypothesis set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias-Variance Tradeoff\n",
    "\n",
    "** For Regression Problem Settings **\n",
    "\n",
    "Starts from Mean Squared Error, with training input D:\n",
    "\n",
    "$$E_{out}(g_D) = \\mathbb E_{\\overrightarrow{x}\\sim P}(g_D(\\overrightarrow{x}) - f(\\overrightarrow{x})^2)$$\n",
    "\n",
    "Thus we have:\n",
    "\n",
    "\n",
    "$$\\mathbb E[E_{out}(g)] = \\mathbb E_{\\overrightarrow x}[\\mathbb E_D[g_D(\\overrightarrow x)^2 - \\bar g(\\overrightarrow{x})^2] + (\\bar g(\\overrightarrow{x}) - f(\\overrightarrow{x})^2]$$\n",
    "\n",
    "$$= \\mathbb E_{\\overrightarrow x}[Variance\\space of\\space g_D(\\overrightarrow x) + Bias\\space of\\space \\bar g(\\overrightarrow{x})]$$\n",
    "\n",
    "\n",
    "**Bias**\n",
    "\n",
    "How different between the average of our hypothesis set and the true function f\n",
    "\n",
    "(How well, on average, does g approximate f?)\n",
    "\n",
    "\n",
    "**Vairance**\n",
    "\n",
    "How variable our whole dataset compared to the average of our hypothesis set\n",
    "\n",
    "(How well could g approximate anything? How much could noise affect g?)\n",
    "\n",
    "\n",
    "**More complicated Hypothesis**\n",
    "\n",
    "Bias decreases, Variance increases\n",
    "\n",
    "\n",
    "**More data brought to exsisting Hypothesis**\n",
    "\n",
    "Bias is fixed when model was chosen, so no change for Bias;\n",
    "\n",
    "Variance will decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "## Maximum Likelihood Estimation \n",
    "\n",
    "Maximum Likelihood Estimation (MLE) is a method of estimating the parameters of a distribution by maximizing a likelihood function, so that under the assumed statistical model the observed data is most probable. \n",
    "\n",
    "** Finding the parameter values that maximize the likelihood of making the observations given the parameters.**\n",
    "\n",
    "\n",
    "**Assumption for Linear Regression** \n",
    "\n",
    "$$Y \\sim Normal ( \\overrightarrow {\\beta}\\overrightarrow X_i, \\sigma ^2)$$\n",
    "\n",
    "$$ X \\sim Normal (\\mu, \\sigma^2)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Probability Density Function for Normal Distribution**\n",
    "\n",
    "$$P(x_i\\mid \\mu,\\sigma^2)={\\frac {1}{\\sqrt {2\\pi \\sigma ^{2}}}}e^{-{\\frac {(x_i-\\mu )^{2}}{2\\sigma ^{2}}}}$$\n",
    "\n",
    "** Bayes's Theorem **\n",
    "\n",
    "$$ P(\\theta \\mid X)={\\frac {P(X \\mid \\theta)P(\\theta)}{P(X)}}\\cdot $$\n",
    "\n",
    "where likelihood funtion is $P(X \\mid \\theta)$ and prior distribution is $p(\\theta )$\n",
    "\n",
    "** For a Dataset, if independence holds **\n",
    "\n",
    "$$ L = P(\\overrightarrow X \\mid \\theta) = P(x_1,x_2,...x_n  \\mid \\theta) = P(x_1\\mid \\theta)P(x_2 \\mid \\theta)P(x_3\\mid \\theta)...P(x_n\\mid \\theta) = \\prod_{i=1} ^n P(x_i \\mid \\theta) $$\n",
    "\n",
    "substitute for normal PDF\n",
    "\n",
    "$$ L = \\frac{1}{\\sigma ^n(2\\pi)^{\\frac{n}{2}}} e^{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(Y_i-\\hat \\beta_0 - \\hat \\beta_1 X_i)^2}$$\n",
    "\n",
    "$$ \\ln(L) = -n\\ln(\\sqrt {2\\pi} \\sigma) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n(Y_i- \\overrightarrow {\\beta}\\overrightarrow X_i)^2$$\n",
    "\n",
    "In order to maximize $\\ln(L)$:\n",
    "\n",
    "$$argmax_\\beta[- \\sum_{i=1}^n(Y_i- \\overrightarrow {\\beta}\\overrightarrow X_i)^2]$$\n",
    "\n",
    "Thus, $argmin_\\beta[\\sum_{i=1}^n(Y_i-\\overrightarrow {\\beta}\\overrightarrow X_i)^2]$\n",
    "\n",
    "*which is also the squared error*\n",
    "\n",
    "In order to find the optima, we can set the first derivative equals to zero, find the weights, and then check the sign using second derivative\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Squared Error and Vectorized Optimization\n",
    "\n",
    "Similarily as MLE methods, we want to optimize parameters to have lowest mean squared error.\n",
    "\n",
    "Let's try a vectorized optimization:\n",
    "\n",
    "$$ E_{in}(\\overrightarrow \\theta)=\\frac{1}{n}\\sum_{i=1}^n(\\overrightarrow {\\theta}^T \\overrightarrow {x_i} - y_i)^2 = \\frac{1}{n}\\sum_{i=1}^n(\\overrightarrow {x_i}^T \\overrightarrow {\\theta} - y_i)^2 = \\frac{1}{n}\\mid\\mid X\\overrightarrow w - \\overrightarrow y \\mid \\mid ^2$$\n",
    "\n",
    "$$ since \\mid \\mid \\overrightarrow Z \\mid\\mid = \\sqrt{\\sum_{i=1}^d z_i^2} = \\sqrt{\\overrightarrow Z^T \\overrightarrow Z}$$\n",
    "\n",
    "$$ E_{in}(\\overrightarrow \\theta)= \\frac{1}{n}(X\\overrightarrow \\theta - \\overrightarrow y)^T(X\\overrightarrow \\theta - \\overrightarrow y)$$\n",
    "\n",
    "$$ \\Delta_\\theta E_{in}(\\overrightarrow \\theta^*) = \\frac{1}{n}(2X^TX\\overrightarrow \\theta^* - 2X^T\\overrightarrow y) = 0$$\n",
    "\n",
    "$$\\overrightarrow \\theta^* = (X^TX)^{-1}X^T\\overrightarrow y$$\n",
    "\n",
    "*when the number of parameter << the dataset, we can assume X is transferable*\n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "**Reference**\n",
    "\n",
    "Learning From Data (http://amlbook.com/)\n",
    "\n",
    "Maximum Likelihood Wikipedia https://en.wikipedia.org/wiki/Maximum_likelihood_estimation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
