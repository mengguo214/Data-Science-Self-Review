{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Learning is Feasible?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Hoeffding Inequality\n",
    "\n",
    "\n",
    "For a Single Hypothesis h, the Hoeffding Inequality can be used:\n",
    "\n",
    "\n",
    "$$P(\\mid E_{in}(h) - E_{out}(h) \\mid > \\epsilon) \\leq 2e^{-2\\epsilon^2N}$$ for any $\\epsilon > 0$, $N$ is the number of training examples\n",
    "\n",
    "\n",
    "The h is fixed before generating the whole dataset. If you want to change h later, then the Hoeffding Inequality no longer holds.\n",
    "\n",
    "With multiple hypotheses $H = \\{h_1, h_2, h_3, ..., h_M\\}$, the algorithm needs to pick a hypothesis based on the data - in this way, the g is picked after the generation of dataset. In order to bound it, we don't want the choice of g affects our bound, thus we want this to be true:\n",
    "\n",
    "$$\"\\mid E_{in}(h) - E_{out}(h) \\mid > \\epsilon\" \\   => $$\n",
    "\n",
    "\n",
    "$$\\  \\mid E_{in}(h_1) - E_{out}(h_1) \\mid > \\epsilon \\ or \\mid E_{in}(h_2) - E_{out}(h_2) \\mid > \\epsilon \\  or ... \\  or \\mid E_{in}(h_M) - E_{out}(h_M) \\mid > \\epsilon$$\n",
    "\n",
    "\n",
    "or can be expressed as Union Bound:\n",
    "\n",
    "$$\\mid E_{in}(h_1) - E_{out}(h_1) \\mid > \\epsilon \\ \\bigcup \\mid E_{in}(h_2) - E_{out}(h_2) \\mid > \\epsilon \\  \\bigcup ... \\bigcup \\mid E_{in}(h_M) - E_{out}(h_M) \\mid > \\epsilon$$\n",
    "\n",
    "\n",
    "$$\\mid E_{in}(h_1) - E_{out}(h_1) \\mid > \\epsilon \\ + \\mid E_{in}(h_2) - E_{out}(h_2) \\mid > \\epsilon \\  + ... \\  + \\mid E_{in}(h_M) - E_{out}(h_M) \\mid > \\epsilon$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$P(\\mid E_{in}(g) - E_{out}(g) \\mid > \\epsilon) \\leq 2Me^{-2\\epsilon^2N}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feasibility of Learning\n",
    "\n",
    "1. Can we make sure that $E_{out}(g)$ is close enough to $E_{in}(g)$ ?\n",
    "2. Can we make $E_{in}(g)$ small enough?\n",
    "\n",
    "The Hoeffding Inequality addresses the first question only. The second question is answered after we run the learning algorithm on the actual data and see how small we can get $E_{in}(g)$ to be.\n",
    "\n",
    "The two questions provides further insight into the role about \"complexity\", which leads us to the discussion about generaliztion tradeoff:\n",
    "\n",
    "The more complex of $H$, $M$ becomes bigger, thus we run more risk that $E_{in}(g)$ will be a poor estimator of $E_{out}(g)$\n",
    "\n",
    "But we stand a better chance that $E_{in}(g)$ could be small, since $g$ has to come from $H$\n",
    "\n",
    "Even though the complexity of $f(x)$ doesn't affect Hoeffding Inequality, $E_{in}$ is more likely to be worse when $f(x)$ is more complex\n",
    "\n",
    "\n",
    "## Noise\n",
    "\n",
    "When there is no noise, then we can use the true function $f(x)$ as our target function.\n",
    "\n",
    "BUT When noise exsists, formally we don't use the true function $f(x)$ as our target function. Instead, we are using $P(y\\mid x)$, and the datapoint are be represented as $P(x,y) = P(y\\mid x) * P(x) $\n",
    "\n",
    "The noisy target function is composed of two things:\n",
    "\n",
    "1. deterministic target: $E(y\\mid x)$\n",
    "2. random noise: $y - f(x)$\n",
    "\n",
    "the deterministic target can be also viewed as a special case of noisy target function where noise = 0. In other words, we can express $f$ as a distribution of $P(y\\mid x)$ by choosing $P(y\\mid x)$ equals to zero except $y = f(x)$\n",
    "\n",
    "\n",
    "Even though we don't loose any generality if we consider the target to be a distribution rather than a function, $E_{in}(g)$ is more likely to be worse when the noise presents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Approximation Generalization Tradeoff\n",
    "\n",
    "** For Classification Problem Settings **\n",
    "\n",
    "\n",
    "$$E_{out} (g) \\leqslant E_{in} (g) + \\mathcal{O}(\\sqrt{d_{vc}\\frac{log(n)}{n}})$$\n",
    "\n",
    "**$E_{in} (g)$ decreases as $d_{vc}$ increases**;\n",
    "\n",
    "**$\\mathcal{O}(\\sqrt{d_{vc}\\frac{log(n)}{n}})$ increases as $d_{vc}$ increases**.\n",
    "\n",
    "\n",
    "$d_{vc}$ represents VC-Dimension, the greatest number of points that can be\n",
    "shattered by Hypothesis set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias-Variance Tradeoff\n",
    "\n",
    "** For Regression Problem Settings **\n",
    "\n",
    "Starts from Mean Squared Error, with training input D:\n",
    "\n",
    "$$E_{out}(g_D) = \\mathbb E_{\\overrightarrow{x}\\sim P}(g_D(\\overrightarrow{x}) - f(\\overrightarrow{x})^2)$$\n",
    "\n",
    "Thus we have:\n",
    "\n",
    "\n",
    "$$\\mathbb E[E_{out}(g)] = \\mathbb E_{\\overrightarrow x}[\\mathbb E_D[g_D(\\overrightarrow x)^2 - \\bar g(\\overrightarrow{x})^2] + (\\bar g(\\overrightarrow{x}) - f(\\overrightarrow{x})^2]$$\n",
    "\n",
    "$$= \\mathbb E_{\\overrightarrow x}[Variance\\space of\\space g_D(\\overrightarrow x) + Bias\\space of\\space \\bar g(\\overrightarrow{x})]$$\n",
    "\n",
    "\n",
    "**Bias**\n",
    "\n",
    "How different between the average of our hypothesis set and the true function f\n",
    "\n",
    "(How well, on average, does g approximate f?)\n",
    "\n",
    "\n",
    "**Vairance**\n",
    "\n",
    "How variable our whole dataset compared to the average of our hypothesis set\n",
    "\n",
    "(How well could g approximate anything? How much could noise affect g?)\n",
    "\n",
    "\n",
    "**More complicated Hypothesis**\n",
    "\n",
    "Bias decreases, Variance increases\n",
    "\n",
    "\n",
    "**More data brought to exsisting Hypothesis**\n",
    "\n",
    "Bias is fixed when model was chosen, so no change for Bias;\n",
    "\n",
    "Variance will decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "## Maximum Likelihood Estimation \n",
    "\n",
    "Maximum Likelihood Estimation (MLE) is a method of estimating the parameters of a distribution by maximizing a likelihood function, so that under the assumed statistical model the observed data is most probable. \n",
    "Finding the parameter values that maximize the likelihood of making the observations given the parameters.\n",
    "\n",
    "\n",
    "**Assumption for Linear Regression - Gaussian Noise Distribution** \n",
    "\n",
    "1. The distribution of $X$ is arbitrary \n",
    "2. If $\\overrightarrow X$ and $\\overrightarrow \\beta$ , then $Y = \\overrightarrow \\beta * \\overrightarrow X + \\epsilon$\n",
    "3. $\\epsilon \\sim Normal (0, \\sigma ^2)$\n",
    "4. $\\epsilon$ is independent across observations\n",
    "5. $Y$ is independent across observations give $X_i$\n",
    "6. Then ($Y$ given X) $\\sim Normal ( {\\hat {\\beta}\\overrightarrow X_i}, \\sigma ^2)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Probability Density Function for Normal Distribution**\n",
    "\n",
    "$$P(x_i\\mid \\mu,\\sigma^2)={\\frac {1}{\\sqrt {2\\pi \\sigma ^{2}}}}e^{-{\\frac {(x_i-\\mu )^{2}}{2\\sigma ^{2}}}}$$\n",
    "\n",
    "** Bayes's Theorem **\n",
    "\n",
    "$$ P(\\theta \\mid X)={\\frac {P(X \\mid \\theta)P(\\theta)}{P(X)}}\\cdot $$\n",
    "\n",
    "where likelihood funtion is $P(X \\mid \\theta)$ and prior distribution is $p(\\theta )$\n",
    "\n",
    "** For a Dataset, if independence holds **\n",
    "\n",
    "$$ L = P(Y_i \\mid \\overrightarrow X) = P(Y_1,Y_2,...Y_n  \\mid \\overrightarrow X) = P(Y_1\\mid x_1)P(Y_2 \\mid x_2)P(Y_3\\mid x_3)...P(Y_n\\mid x_n) = \\prod_{i=1} ^n P(Y_i \\mid x_i) $$\n",
    "\n",
    "substitute for normal PDF\n",
    "\n",
    "$$ L = \\frac{1}{\\sigma ^n(2\\pi)^{\\frac{n}{2}}} e^{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(\\overrightarrow {\\beta}\\overrightarrow X_i)}$$\n",
    "\n",
    "$$ \\ln(L) = -n\\ln(\\sqrt {2\\pi} \\sigma) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n(Y_i- \\overrightarrow {\\beta}\\overrightarrow X_i)^2$$\n",
    "\n",
    "In order to maximize $\\ln(L)$:\n",
    "\n",
    "$$argmax_\\beta[- \\sum_{i=1}^n(Y_i- \\overrightarrow {\\beta}\\overrightarrow X_i)^2]$$\n",
    "\n",
    "Thus, $argmin_\\beta[\\sum_{i=1}^n(Y_i-\\overrightarrow {\\beta}\\overrightarrow X_i)^2]$\n",
    "\n",
    "*which is also the squared error*\n",
    "\n",
    "In order to find the optima, we can set the first derivative equals to zero, find the weights, and then check the sign using second derivative\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Squared Error and Vectorized Optimization\n",
    "\n",
    "Similarily as MLE methods, we want to optimize parameters to have lowest mean squared error.\n",
    "\n",
    "Let's try a vectorized optimization:\n",
    "\n",
    "$$ E_{in}(\\overrightarrow \\beta)=\\frac{1}{n}\\sum_{i=1}^n(\\overrightarrow {\\beta}^T \\overrightarrow {x_i} - Y_i)^2 = \\frac{1}{n}\\sum_{i=1}^n(\\overrightarrow {x_i}^T \\overrightarrow {\\beta} - Y_i)^2 = \\frac{1}{n}\\mid\\mid X\\overrightarrow w - \\overrightarrow Y \\mid \\mid ^2$$\n",
    "\n",
    "$$ since \\mid \\mid \\overrightarrow Z \\mid\\mid = \\sqrt{\\sum_{i=1}^d z_i^2} = \\sqrt{\\overrightarrow Z^T \\overrightarrow Z}$$\n",
    "\n",
    "$$ E_{in}(\\overrightarrow \\beta)= \\frac{1}{n}(X\\overrightarrow \\beta - \\overrightarrow Y)^T(X\\overrightarrow \\beta - \\overrightarrow Y)$$\n",
    "\n",
    "$$ \\Delta_\\beta E_{in}(\\overrightarrow \\beta^*) = \\frac{1}{n}(2X^TX\\overrightarrow \\beta^* - 2X^T\\overrightarrow Y) = 0$$\n",
    "\n",
    "$$\\overrightarrow \\beta^* = (X^TX)^{-1}X^T\\overrightarrow Y$$\n",
    "\n",
    "*when the number of parameter << the dataset, we can assume X is transferable*\n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "**Reference**\n",
    "\n",
    "Learning From Data (http://amlbook.com/)\n",
    "\n",
    "Maximum Likelihood Wikipedia https://en.wikipedia.org/wiki/Maximum_likelihood_estimation\n",
    "\n",
    "Estimating simple linear regression II https://www.stat.cmu.edu/~cshalizi/mreg/15/lectures/06/lecture-06.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
