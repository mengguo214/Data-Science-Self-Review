{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Learning Problem\n",
    "\n",
    "1. $E_{in}$ based on data points that have been used for training\n",
    "2. $E_{out}$ based on performance over the entire input space\n",
    "3. $E_{test}$ :If the size of test becomes large,  $E_{test}$ will be close to $E_{out}$. \n",
    "\n",
    "\n",
    "( After later study, we know that we can use Hoeffding Inequality of single hypothesis to bound $E_{test}$, it's a lot tighter bound. Thus it can be useful to estimate $E_{out}$ when the size of test set is large. But a good estimation will not give us any improvement on our hypothesis - a trade-off here is that smaller training set gives a worse $E_{in}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Hoeffding Inequality\n",
    "\n",
    "\n",
    "For a Single Hypothesis h, the Hoeffding Inequality can be used:\n",
    "\n",
    "\n",
    "$$P(\\mid E_{in}(h) - E_{out}(h) \\mid > \\epsilon) \\leq 2e^{-2\\epsilon^2N}$$ for any $\\epsilon > 0$, $N$ is the number of training examples\n",
    "\n",
    "\n",
    "The h is fixed before generating the whole dataset. If you want to change h later, then the Hoeffding Inequality no longer holds.\n",
    "\n",
    "With multiple hypotheses $H = \\{h_1, h_2, h_3, ..., h_M\\}$, the algorithm needs to pick a hypothesis based on the data - in this way, the g is picked after the generation of dataset. In order to bound it, we don't want the choice of g affects our bound, thus we want this to be true:\n",
    "\n",
    "$$\"\\mid E_{in}(h) - E_{out}(h) \\mid > \\epsilon\" \\   => $$\n",
    "\n",
    "\n",
    "$$\\  \\mid E_{in}(h_1) - E_{out}(h_1) \\mid > \\epsilon \\ or \\mid E_{in}(h_2) - E_{out}(h_2) \\mid > \\epsilon \\  or ... \\  or \\mid E_{in}(h_M) - E_{out}(h_M) \\mid > \\epsilon$$\n",
    "\n",
    "\n",
    "or can be expressed as Union Bound:\n",
    "\n",
    "$$\\mid E_{in}(h_1) - E_{out}(h_1) \\mid > \\epsilon \\ \\bigcup \\mid E_{in}(h_2) - E_{out}(h_2) \\mid > \\epsilon \\  \\bigcup ... \\bigcup \\mid E_{in}(h_M) - E_{out}(h_M) \\mid > \\epsilon$$\n",
    "\n",
    "\n",
    "$$\\mid E_{in}(h_1) - E_{out}(h_1) \\mid > \\epsilon \\ + \\mid E_{in}(h_2) - E_{out}(h_2) \\mid > \\epsilon \\  + ... \\  + \\mid E_{in}(h_M) - E_{out}(h_M) \\mid > \\epsilon$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$P(\\mid E_{in}(g) - E_{out}(g) \\mid > \\epsilon) \\leq 2Me^{-2\\epsilon^2N}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Feasibility of Learning\n",
    "\n",
    "1. Can we make sure that $E_{out}(g)$ is close enough to $E_{in}(g)$ ?\n",
    "2. Can we make $E_{in}(g)$ small enough?\n",
    "\n",
    "The Hoeffding Inequality addresses the first question only. The second question is answered after we run the learning algorithm on the actual data and see how small we can get $E_{in}(g)$ to be.\n",
    "\n",
    "The two questions provides further insight into the role about \"complexity\", which leads us to the discussion about generaliztion tradeoff:\n",
    "\n",
    "The more complex of $H$, $M$ becomes bigger, thus we run more risk that $E_{in}(g)$ will be a poor estimator of $E_{out}(g)$\n",
    "\n",
    "But we stand a better chance that $E_{in}(g)$ could be small, since $g$ has to come from $H$\n",
    "\n",
    "Even though the complexity of $f(x)$ doesn't affect Hoeffding Inequality, $E_{in}$ is more likely to be worse when $f(x)$ is more complex\n",
    "\n",
    "\n",
    "## 1.3 Noise\n",
    "\n",
    "When there is no noise, then we can use the true function $f(x)$ as our target function.\n",
    "\n",
    "BUT When noise exsists, formally we don't use the true function $f(x)$ as our target function. Instead, we are using $P(y\\mid x)$, and the datapoint are be represented as $P(x,y) = P(y\\mid x) * P(x) $\n",
    "\n",
    "The noisy target function is composed of two things:\n",
    "\n",
    "1. deterministic target: $E(y\\mid x)$\n",
    "2. random noise: $y - f(x)$\n",
    "\n",
    "the deterministic target can be also viewed as a special case of noisy target function where noise = 0. In other words, we can express $f$ as a distribution of $P(y\\mid x)$ by choosing $P(y\\mid x)$ equals to zero except $y = f(x)$\n",
    "\n",
    "\n",
    "Even though we don't loose any generality if we consider the target to be a distribution rather than a function, $E_{in}(g)$ is more likely to be worse when the noise presents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Theory of Generalization\n",
    "\n",
    "## 2.1 Starts From Hoeffding Inequality \n",
    "$$P(\\mid E_{in}(g) - E_{out}(g) \\mid > \\epsilon) \\leq 2Me^{-2\\epsilon^2N}$$\n",
    "\n",
    "If we pick a tolerance level $\\delta$:\n",
    "\n",
    "$$ E_{out} \\leq E_{in}(g)  + \\sqrt{\\frac{1}{2N}\\ln{\\frac{2M}{\\delta}}}$$\n",
    "\n",
    "**BUT** It's a very loose bound not only because in real-life $M$ always be $\\infty $, but also there are lots of overlap hypothesis in $H$\n",
    "\n",
    "So, we want to find a **Growth Function** to bound $M$\n",
    "\n",
    "## 2.2 Effective Number of Hypothesis\n",
    "\n",
    "1. shatter: If $H$ is capable of generating all possible dichotomies on $x_1, ... x_N$, then $H$ can shatter $(x_1, ... x_N)$\n",
    "\n",
    "$$m_H(N) \\leq 2^N$$\n",
    "\n",
    "2. break point: If *no data set of size $k$* can be shattered by $H$, then $k$ is said to be a break point for $H$ \n",
    "\n",
    "$$m_H(k) < 2^k$$\n",
    "\n",
    "it can also be proved that if $m_H(k) < 2^k$, then for all $N$:\n",
    "\n",
    "$$m_H(N) \\leq \\sum_{i=0}^{k-1}{N\\choose i} \\leq N^{k-1}$$\n",
    "\n",
    "\n",
    "\n",
    "## 2.3 VC Bound \n",
    "\n",
    "### 2.3.1 VC Dimension\n",
    "\n",
    "The Vapnik-Chervonenkis dimension of a hypothesis set $H$ - $d_{vc}(H)$ is the largest value of $N$ for which $m_H(N) == 2^N$. Thus if k is a break point, $d_{vc}(H) = k-1 $:\n",
    "\n",
    "$$m_H(N) \\leq \\sum_{i=0}^{d_{vc}(H)}{N\\choose i} \\leq N^{d_{vc}(H)}$$\n",
    "\n",
    "We can say that if we have a finite $d_{vc}(H)$, then as $N$ becomes close to $\\infty$, $ \\sqrt{\\frac{1}{2N}\\ln{\\frac{2M}{\\delta}}}$ will converge to 0, which means $E_{in}$ becomes close to $E_{out}$\n",
    "\n",
    "But if $d_{vc}(H)$ is infinite, then there is no guarantee that $E_{in}$ and $E_{out}$ could be close\n",
    "\n",
    "\n",
    "\n",
    "### 2.3.2 VC Generalization Bound\n",
    "\n",
    "After prove, substitute $M$ for $m_H$ and some change (to make sure the form holds):\n",
    "$$ E_{out}(g) \\leq E_{in}(g)  + \\sqrt{\\frac{8}{N}\\ln{\\frac{4m_H(2N)}{\\delta}}}$$\n",
    "with probability $\\delta$\n",
    "\n",
    "$$ E_{out}(g) \\leq E_{in}(g)  + \\sqrt{\\frac{8}{N}\\ln{\\frac{4(2N)^{d_{vc}}}{\\delta}}}$$\n",
    "\n",
    "It's loose thus we can:\n",
    "\n",
    "1. Estabilish the feasibility of learning with infinite hypothesis sets -> finite vc dimension\n",
    "2. Useful for comparing the generalization performance of different models\n",
    "\n",
    "We can also compute the size of data set we need, for a given tolerance level $\\delta$ and $\\epsilon$:\n",
    "\n",
    "$$N \\geq \\frac{8}{\\epsilon^2}\\ln(\\frac{4m_H(2N)}{\\delta})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 3. Approximation Generalization Tradeoff\n",
    "\n",
    "## 3.1 Classification Problem Setting\n",
    "\n",
    "\n",
    "$$E_{out} (g) \\leqslant E_{in} (g) + \\mathcal{O}(\\sqrt{d_{vc}\\frac{log(n)}{n}})$$\n",
    "\n",
    "**$E_{in} (g)$ decreases as $d_{vc}$ increases**;\n",
    "\n",
    "**$\\mathcal{O}(\\sqrt{d_{vc}\\frac{log(n)}{n}})$ increases as $d_{vc}$ increases**.\n",
    "\n",
    "\n",
    "$d_{vc}$ represents VC-Dimension, the greatest number of points that can be\n",
    "shattered by Hypothesis set\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Bias-Variance Tradeoff\n",
    "\n",
    "** For Regression Problem Settings **\n",
    "\n",
    "Starts from Mean Squared Error, with training input D:\n",
    "\n",
    "$$E_{out}(g_D) = \\mathbb E_{\\overrightarrow{x}\\sim P}(g_D(\\overrightarrow{x}) - f(\\overrightarrow{x})^2)$$\n",
    "\n",
    "Thus we have:\n",
    "\n",
    "\n",
    "$$\\mathbb E[E_{out}(g)] = \\mathbb E_{\\overrightarrow x}[\\mathbb E_D[g_D(\\overrightarrow x) - \\bar g(\\overrightarrow{x})^2] + (\\bar g(\\overrightarrow{x}) - f(\\overrightarrow{x}))^2]$$\n",
    "\n",
    "$$= \\mathbb E_{\\overrightarrow x}[Variance\\space of\\space g_D(\\overrightarrow x) + Bias\\space of\\space \\bar g(\\overrightarrow{x})]$$\n",
    "\n",
    "\n",
    "**Bias**\n",
    "\n",
    "How different between the average of our hypothesis set and the true function f\n",
    "\n",
    "(How well, on average, does g approximate f?)\n",
    "\n",
    "\n",
    "**Vairance**\n",
    "\n",
    "How variable our whole dataset compared to the average of our hypothesis set\n",
    "\n",
    "(How well could g approximate anything? How much could noise affect g?)\n",
    "\n",
    "\n",
    "**More complicated Hypothesis**\n",
    "\n",
    "Bias decreases, Variance increases\n",
    "\n",
    "\n",
    "**More data brought to exsisting Hypothesis**\n",
    "\n",
    "Bias is fixed when model was chosen, so no change for Bias;\n",
    "\n",
    "Variance will decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "## Maximum Likelihood Estimation \n",
    "\n",
    "Maximum Likelihood Estimation (MLE) is a method of estimating the parameters of a distribution by maximizing a likelihood function, so that under the assumed statistical model the observed data is most probable. \n",
    "Finding the parameter values that maximize the likelihood of making the observations given the parameters.\n",
    "\n",
    "\n",
    "**Assumption for Linear Regression - Gaussian Noise Distribution** \n",
    "\n",
    "1. The distribution of $X$ is arbitrary \n",
    "2. If $\\overrightarrow X$ and $\\overrightarrow \\beta$ , then $Y = \\overrightarrow \\beta * \\overrightarrow X + \\epsilon$\n",
    "3. $\\epsilon \\sim Normal (0, \\sigma ^2)$\n",
    "4. $\\epsilon$ is independent across observations\n",
    "5. $Y$ is independent across observations give $X_i$\n",
    "6. Then ($Y$ given X) $\\sim Normal ( {\\hat {\\beta}\\overrightarrow X_i}, \\sigma ^2)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Probability Density Function for Normal Distribution**\n",
    "\n",
    "$$P(x_i\\mid \\mu,\\sigma^2)={\\frac {1}{\\sqrt {2\\pi \\sigma ^{2}}}}e^{-{\\frac {(x_i-\\mu )^{2}}{2\\sigma ^{2}}}}$$\n",
    "\n",
    "** Bayes's Theorem **\n",
    "\n",
    "$$ P(\\theta \\mid X)={\\frac {P(X \\mid \\theta)P(\\theta)}{P(X)}}\\cdot $$\n",
    "\n",
    "where likelihood funtion is $P(X \\mid \\theta)$ and prior distribution is $p(\\theta )$\n",
    "\n",
    "** For a Dataset, if independence holds **\n",
    "\n",
    "$$ L = P(Y_i \\mid \\overrightarrow X) = P(Y_1,Y_2,...Y_n  \\mid \\overrightarrow X) = P(Y_1\\mid x_1)P(Y_2 \\mid x_2)P(Y_3\\mid x_3)...P(Y_n\\mid x_n) = \\prod_{i=1} ^n P(Y_i \\mid x_i) $$\n",
    "\n",
    "substitute for normal PDF\n",
    "\n",
    "$$ L = \\frac{1}{\\sigma ^n(2\\pi)^{\\frac{n}{2}}} e^{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(\\overrightarrow {\\beta}\\overrightarrow X_i)}$$\n",
    "\n",
    "$$ \\ln(L) = -n\\ln(\\sqrt {2\\pi} \\sigma) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n(Y_i- \\overrightarrow {\\beta}\\overrightarrow X_i)^2$$\n",
    "\n",
    "In order to maximize $\\ln(L)$:\n",
    "\n",
    "$$argmax_\\beta[- \\sum_{i=1}^n(Y_i- \\overrightarrow {\\beta}\\overrightarrow X_i)^2]$$\n",
    "\n",
    "Thus, $argmin_\\beta[\\sum_{i=1}^n(Y_i-\\overrightarrow {\\beta}\\overrightarrow X_i)^2]$\n",
    "\n",
    "*which is also the squared error*\n",
    "\n",
    "In order to find the optima, we can set the first derivative equals to zero, find the weights, and then check the sign using second derivative\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Squared Error and Vectorized Optimization\n",
    "\n",
    "Similarily as MLE methods, we want to optimize parameters to have lowest mean squared error.\n",
    "\n",
    "Let's try a vectorized optimization:\n",
    "\n",
    "$$ E_{in}(\\overrightarrow \\beta)=\\frac{1}{n}\\sum_{i=1}^n(\\overrightarrow {\\beta}^T \\overrightarrow {x_i} - Y_i)^2 = \\frac{1}{n}\\sum_{i=1}^n(\\overrightarrow {x_i}^T \\overrightarrow {\\beta} - Y_i)^2 = \\frac{1}{n}\\mid\\mid X\\overrightarrow w - \\overrightarrow Y \\mid \\mid ^2$$\n",
    "\n",
    "$$ since \\mid \\mid \\overrightarrow Z \\mid\\mid = \\sqrt{\\sum_{i=1}^d z_i^2} = \\sqrt{\\overrightarrow Z^T \\overrightarrow Z}$$\n",
    "\n",
    "$$ E_{in}(\\overrightarrow \\beta)= \\frac{1}{n}(X\\overrightarrow \\beta - \\overrightarrow Y)^T(X\\overrightarrow \\beta - \\overrightarrow Y)$$\n",
    "\n",
    "$$ \\Delta_\\beta E_{in}(\\overrightarrow \\beta^*) = \\frac{1}{n}(2X^TX\\overrightarrow \\beta^* - 2X^T\\overrightarrow Y) = 0$$\n",
    "\n",
    "$$\\overrightarrow \\beta^* = (X^TX)^{-1}X^T\\overrightarrow Y$$\n",
    "\n",
    "*when the number of parameter << the dataset, we can assume X is transferable*\n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "**Reference**\n",
    "\n",
    "Learning From Data (http://amlbook.com/)\n",
    "\n",
    "Maximum Likelihood Wikipedia https://en.wikipedia.org/wiki/Maximum_likelihood_estimation\n",
    "\n",
    "Estimating simple linear regression II https://www.stat.cmu.edu/~cshalizi/mreg/15/lectures/06/lecture-06.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
