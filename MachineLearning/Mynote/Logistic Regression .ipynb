{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "## Binary Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivation Process\n",
    "Assume all $X_i$ are conditionally independent given Y,\n",
    "$X$ is a vector of $<x_1, x_2, x_3...x_n>$\n",
    "\n",
    "Then, Model $P(X_i \\mid Y=y_k)$ as Gaussian $N(\\mu_{ik}, \\sigma_i)$\n",
    "AND Model $P(Y)$ as Bernoulli($\\pi$)\n",
    "\n",
    "\n",
    "We can imply that\n",
    "\n",
    "$$ P(Y=1 \\mid X) = \\frac{P(X \\mid Y=1)P(Y=1)}{P(X)} = \\frac{P(X \\mid Y=1)P(Y=1)}{P(X \\mid Y=1)P(Y=1)+P(X \\mid Y=0)P(Y=0)} = \\frac{1}{1+\\frac{P(X \\mid Y=0)P(Y=0}{P(X \\mid Y=1)P(Y=1)}} = \\frac{1}{1+\\exp({\\ln{\\frac{P(X \\mid Y=0)P(Y=0)}{P(X \\mid Y=1)P(Y=1)})}}}$$\n",
    "$$= \\frac{1}{1+\\exp({\\ln{\\frac{1-\\pi}{\\pi}})}+exp{(\\sum_{i=1}^n \\ln{\\frac{P(X_i \\mid Y=0)}{P(X_i \\mid Y=1))})}}}$$\n",
    "\n",
    "Substitute for Gaussian PDF\n",
    "\n",
    "$$P(x_i\\mid y_k, \\mu_{ik},\\sigma_{ik}^2)={\\frac {1}{\\sqrt {2\\pi \\sigma_{ik} ^{2}}}}e^{-{\\frac {(x_i-\\mu_{ik} )^{2}}{2\\sigma_{ik} ^{2}}}}$$\n",
    "\n",
    "$$ P(Y=1 \\mid X) = \\frac{1}{1+\\exp({-\\theta_0 - \\sum_{i=1}^n \\theta_iX_i})}$$\n",
    "\n",
    "Which implies:\n",
    "\n",
    "$$ P(Y=0 \\mid X) = \\frac{\\exp({-\\theta_0 - \\sum_{i=1}^n \\theta_iX_i})}{1+\\exp({-\\theta_0 - \\sum_{i=1}^n \\theta_iX_i})}$$\n",
    "\n",
    "AND \n",
    "\n",
    "\n",
    "$$ \\frac{P(Y=0 \\mid X)}{P(Y=1 \\mid X)} = \\exp({-\\theta_0 - \\sum_{i=1}^n \\theta_iX_i}) $$\n",
    "\n",
    "So,\n",
    "\n",
    "\n",
    "$$ \\ln{\\frac{P(Y=0 \\mid X)}{P(Y=1 \\mid X)}} = -\\theta_0 - \\sum_{i=1}^n \\theta_iX_i $$   **, which is Generilized Linear Model**\n",
    "\n",
    "- *Definition of Ordinary Linear Model*: \n",
    "\n",
    "The conditional mean of Y on X is a linear expression of X with some coefficient $E(Y \\mid X) = X\\theta$\n",
    "\n",
    "- *Definition of Generalized Linear Model*: \n",
    "\n",
    "The conditional mean of Y on X is a linear expression of X with some coefficient after some transformation,  $E(Y \\mid X) = g^{-1}X\\theta$\n",
    "\n",
    "( $g$ is called link function)\n",
    "\n",
    "\n",
    "** $ \\frac{1}{\\exp({-\\theta_0 - \\sum_{i=1}^n \\theta_iX_i})}$ Also named as Sigmoid Function**\n",
    "\n",
    "**Logit is log-odds**\n",
    "\n",
    "**Odds = $P(Y=1 \\mid x)/ (1-P(Y=1 \\mid x))$ **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of Logistic Regression\n",
    "\n",
    "$$F(x) = P(Y=1 \\mid X) = \\frac{1}{1+\\exp({-\\theta_0 - \\sum_{i=1}^n \\theta_iX_i})}$$\n",
    "\n",
    "Measures the probability of $Y=1$ given $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above we know:\n",
    "$P(Y)$ as Bernoulli($\\pi$)\n",
    "\n",
    "$$ P(Y=1 \\mid X) = \\frac{1}{1+\\exp({-\\theta_0 - \\sum_{i=1}^n \\theta_iX_i})}$$\n",
    "\n",
    "Thus, $ h_{\\theta}(x_i) = \\frac{1}{1+\\exp({-\\theta_0 - \\sum_{i=1}^n \\theta_iX_i})}$ is a approximation of $\\pi$\n",
    "\n",
    "$$P(Y=y_i \\mid x_i) = \\pi^{y_i}(1-\\pi)^{1-{y_i}}$$\n",
    "\n",
    "$$P(Y=y_i \\mid x_i) = (\\frac{1}{1+\\exp({-\\theta_0 - \\sum_{i=1}^n \\theta_iX_i})})^{y_i}(1-\\frac{1}{1+\\exp({-\\theta_0 - \\sum_{i=1}^n \\theta_iX_i})})^{1-{y_i}} = h_{\\theta}(x_i)^{y_i}(1-h_{\\theta}(x_i))^{1-{y_i}}$$ \n",
    "\n",
    "$$ L = P(Y_i \\mid \\overrightarrow X) = P(Y_1,Y_2,...Y_n  \\mid \\overrightarrow X) = P(Y_1\\mid x_1)P(Y_2 \\mid x_2)P(Y_3\\mid x_3)...P(Y_n\\mid x_n) = \\prod_{i=1} ^n P(Y_i \\mid x_i) $$\n",
    "\n",
    "$$ =  \\prod_{i=1} ^n h_{\\theta}(x_i)^{y_i}(1-h_{\\theta}(x_i))^{1-{y_i}}$$\n",
    "\n",
    "$$ \\ln{L} = \\sum_{i=1}^n [{y_i} \\ln{h_{\\theta}(x_i)}+{(1-{y_i})}ln{(1-h_{\\theta}(x_i))}] $$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$l(\\theta) = \\sum_{i=1}^n [{y_i} \\ln{h_{\\theta}(x_i)}+{(1-{y_i})}ln{(1-h_{\\theta}(x_i))}]$$\n",
    "\n",
    "\n",
    "$$argmax_{\\theta}\\sum_{i=1}^n [{y_i} \\ln{h_{\\theta}(x_i)}+{(1-{y_i})}ln{(1-h_{\\theta}(x_i))}]$$\n",
    "\n",
    "Equals to\n",
    "\n",
    "$$argmin_{\\theta}\\sum_{i=1}^n [-{y_i} \\ln{h_{\\theta}(x_i)}-{(1-{y_i})}ln{(1-h_{\\theta}(x_i))}]$$\n",
    "\n",
    "Which is our objective function\n",
    "\n",
    "Which is also called Cross Entropy Error or Log Loss\n",
    "\n",
    "\n",
    "Good news: the objective function $l(\\theta)$ is convex in $\\theta$\n",
    "\n",
    "\n",
    "Bad news: no closed-form solution to maximize "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ l(\\theta) = \\sum_{i=1}^n [{y_i} \\ln{h_{\\theta}(x_i)}+{(1-{y_i})}ln{(1-h_{\\theta}(x_i))}]\n",
    "= \\sum_{i=1}^n [y_i(\\log{\\frac{1}{1+e^{-\\theta x_i}}} - \\log{\\frac{e^{-\\theta x_i}}{1+e^{-\\theta x_i}}})\n",
    "+ \\log{\\frac{e^{-\\theta x_i}}{1+e^{-\\theta x_i}}}]$$\n",
    "\n",
    "\n",
    "It's hard to use first derivative to find the optimal solution to the objective function.\n",
    "\n",
    "Ways to do:\n",
    "\n",
    "Newton's Method\n",
    "\n",
    "Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
